# General System Parameterizer
Entropy Analysis of Counterfactual Results as Parameterized by Nonparametric Modeling Optimized Through DNNs.

"Causal Awareness as Driven By Feature Importance"

"It is not his possession of knowledge, of irrefutable truth, that makes the man of science, but his persistent and recklessly critical quest for truth." - Karl Popper

“The actuality of the learning process consists in the evolution of individual and subjective beliefs about the reality.” - Adrian Smith

# Premise
In the field of AI we often model phenomena with little ability to determine the casual importance of different features within the overall system. This is useful for solving over parameterized optimization problems, but a capability is missing. Is there a mechanism for characterizing a massively overparamterized problem (such as the outcome of large simulations) in a manner that allows for general metrics, insights and utilities, to be extracted from this model. Further, suppose such determinations are pulled from a simulation, are we then able to use them to grant the system awareness of the implications of its own results by feeding these acknowledgements back into the model in a form of fostered self awareness as the model trains.

In other words, we seek to extend the existing understanding of bayesian distribution modeling using neural network of a variety of different architectures: (https://en.wikipedia.org/wiki/Bayesian_hierarchical_modeling).



Doing so we hope the model will construct what can be characterized as general hypotheses about the data it has ingested and what the outcome will be.

# Installation and Execution

# Formal Method Applied
We begin with an environment and we perform some run through through a temporally isolated set of events and arrive at a metric to evaluate this performance. The result of the joint distribution of all independent runs into a singular distribution reflects the understanding of the model. We then perturbate individual variables and relate their entropy to the model as whole. The result of these indpendent and cumulative pairings construct what we can describe as the entropy decoder/encoder of the overall model. As the model grows dramatically in terms of its complexiy, that is, as the number of variables trends towards infinity, a new approach must be introduced, this approach is the training of model on the construction of its own distributions. This partial\contingent knowledge development is fascinating as it yields access to particular metrics which I have never seen referenced before in this field. The fundamental idea is to combine distributions in a cohesive manner so as to yield independent outcomes by perturbating a single variable and observing outcomes of the whole. This creates counterfactual awareness and lends the overall transparency as the implicit combinaion of explicit distributions.

# Methodology
This is a general question which surely cannot be addressed fully within the confines of a single experiment or even a few experiments, but we will first seek to construct general awareness plots of the resulting entropy of the entire model and tie it back into the overall method. So, we begin, as much of deep learning does, with MNIST in all of its wonderous and frequently overfit glory.

For MNIST we seek to make the model aware of its 

# Hypotehsis
These are predictions which have yet to be demonstrated formally:

Increased probabilistic estimation can be used to reinforce existing predictions across a performance domains.

# Note by the Author
Empirical authority within the field of science is only relatively recent and somewhat illbegotent. Logical empiricism, or else, logical positivism had a burst of tyranical normality within the field of empiricism, only to collapse under its own weight and die a slow death by a thousand cuts. Today most everyone who is not aware of where they stand empirically likely stands where I stand, or close to it, in the strangely clinical world of scientific realism. The need to formalize knowledge may seem to stand against the newfound thesis of implicit association so often lauded by today's deep learning professionals, but it is necessary to disambiguate the distinction between what is implicit representation and explicit understanding. The fear of statements is not something to be feared, it's our currency as computer scientists and the preferred way we have elected to communicate.

We begin with the thesis of gaussian noise. An impossible thesis except for maybe the heat death of the universe. We know nothing, and now we must change something and inject some notion into the darkness. Then we formulate an expectation based on this and further change the situation. It's bayesian isn't it? Exactly what we might expect. We know we have near complete knowledge when expectation and reality begin to coaless. This simple process obscures a great deal of understanding. Veiled by our own implicit understanding the situation is the process by which exploration is met by insight, and converted over to the general impression of utility.

-Patrick
